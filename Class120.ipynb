{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class120.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRYyuyL/0Yy21zNUrhrzxp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshmitaRakshit/Class120/blob/main/Class120.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yKTt6ecaJdz"
      },
      "source": [
        "Naive Baybs algorithm- It assumes that every feature in a dataset is independent and has its own contribution to the outcome.\n",
        "\n",
        "Example-how can you if a vegetable is tomato?\n",
        "\n",
        "if it is round (about 4cm iin diameter),red in color\n",
        "With naive bayes, each of these features (shape, size, color) contribute\n",
        "in independently to the probability that the vegetable is tomato.\n",
        "Also it assumes that there is no possible correlation between the shape, size and color.\n",
        "\n",
        "\n",
        "We have already studied about logistic regression (that predicts the dependency of the variable in a dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXVEN5_fdNbI",
        "outputId": "b770b6e6-ecef-4dec-b5af-5ba35749f558"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"diabetes.csv\")\n",
        "print (df.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   glucose  bloodpressure  diabetes\n",
            "0       40             85         0\n",
            "1       40             92         0\n",
            "2       45             63         1\n",
            "3       45             80         0\n",
            "4       40             73         1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1eZTMiZeMqg"
      },
      "source": [
        "x=df[[\"bloodpressure\",\"glucose\"]]\n",
        "y=df[\"diabetes\"]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TypWfikzemN1"
      },
      "source": [
        "from sklearn .model_selection import train_test_split\n",
        "x_train_1,x_test_1,y_train_1,y_test_1=train_test_split(x,y,test_size=0.25,random_state=45)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB_5LsbofgKJ",
        "outputId": "698555f3-bba1-4d2f-877c-c308c9e92267"
      },
      "source": [
        "#Training the model with naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_1 = sc.fit_transform(x_train_1) \n",
        "x_test_1 = sc.fit_transform(x_test_1) \n",
        "\n",
        "model_1 = GaussianNB()\n",
        "model_1.fit(x_train_1, y_train_1)\n",
        "\n",
        "y_pred_1 = model_1.predict(x_test_1)\n",
        "\n",
        "accuracy = accuracy_score(y_test_1, y_pred_1)\n",
        "print(accuracy)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9397590361445783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46abXymJg2VB"
      },
      "source": [
        "Here, we can see that we have an accuracy of approximately an outstanding 93%. Let's see if using logistics regression would have given us the same accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1IQyJqChDnC"
      },
      "source": [
        "#Lets apply logistic regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"glucose\", \"bloodpressure\"]]\n",
        "y = df[\"diabetes\"]\n",
        "\n",
        "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Im7-ZuhWxK",
        "outputId": "00120655-3283-4cac-a166-55d3402d5168"
      },
      "source": [
        "#Training the model with Logistic Regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_2 = sc.fit_transform(x_train_2) \n",
        "x_test_2 = sc.fit_transform(x_test_2) \n",
        "\n",
        "model_2 = LogisticRegression(random_state = 0) \n",
        "model_2.fit(x_train_2, y_train_2)\n",
        "\n",
        "y_pred_2 = model_2.predict(x_test_2)\n",
        "\n",
        "accuracy = accuracy_score(y_test_2, y_pred_2)\n",
        "print(accuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9156626506024096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQQP_2Qyhrpz"
      },
      "source": [
        "While the accuracy score for both the datasets was close, with Naive Bayes giving us an accuracy of 93% and logistic regression giving us an accuracy of 91.6%, Naive Bayes still performed better.\n",
        "\n",
        "The reason for this is that if we look at our features again, we can see that the Glucose and the Blood Pressure had no correlation with each other. They both contributed individually to whether a person would have diabetes or not. This is exactly what Naive Bayes algorithm assumes, that all the features contribute individually to the outcome.\n",
        "\n",
        "\\ This was for the case of where Naive Bayes outperforms Logistic Regression, but let's see an example of the case where Logistic Regression outperforms Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc9CLb8JiGbq",
        "outputId": "cda07f46-6da6-47e0-fa63-86fba21675d9"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('income.csv')\n",
        "\n",
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age          workclass  ...  native-country  income\n",
            "0   39          State-gov  ...   United-States   <=50K\n",
            "1   50   Self-emp-not-inc  ...   United-States   <=50K\n",
            "2   38            Private  ...   United-States   <=50K\n",
            "3   53            Private  ...   United-States   <=50K\n",
            "4   28            Private  ...            Cuba   <=50K\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "                age  education-num  capital-gain  capital-loss  hours-per-week\n",
            "count  45222.000000   45222.000000  45222.000000  45222.000000    45222.000000\n",
            "mean      38.547941      10.118460   1101.430344     88.595418       40.938017\n",
            "std       13.217870       2.552881   7506.430084    404.956092       12.007508\n",
            "min       17.000000       1.000000      0.000000      0.000000        1.000000\n",
            "25%       28.000000       9.000000      0.000000      0.000000       40.000000\n",
            "50%       37.000000      10.000000      0.000000      0.000000       40.000000\n",
            "75%       47.000000      13.000000      0.000000      0.000000       45.000000\n",
            "max       90.000000      16.000000  99999.000000   4356.000000       99.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X0WOu0oidnm"
      },
      "source": [
        "From the given data, we will consider the following fields to determine the salary of a person -\n",
        "\n",
        "\\\n",
        "Age\n",
        "\\\n",
        "Hours Per Week\n",
        "\\\n",
        "Education Number\n",
        "\\\n",
        "Capital Gain\n",
        "\\\n",
        "Capital Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C3bC7ULi0VA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]]\n",
        "y = df[\"income\"]\n",
        "\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eddeHxgi8qs",
        "outputId": "9ea661aa-56f9-43a2-965c-bd2a9004df63"
      },
      "source": [
        "#Training the model with Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_1 = sc.fit_transform(x_train_1) \n",
        "x_test_1 = sc.fit_transform(x_test_1) \n",
        "\n",
        "model_1 = GaussianNB()\n",
        "model_1.fit(x_train_1, y_train_1)\n",
        "\n",
        "y_pred_1 = model_1.predict(x_test_1)\n",
        "\n",
        "accuracy = accuracy_score(y_test_1, y_pred_1)\n",
        "print(accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7896692021935255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WjOcFGxjJoB"
      },
      "source": [
        "This time, with the new dataset, we can see that Naive Bayes gave us an accuracy of almost 79%. Let's see how much accuracy do we get with Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a72acOkfjMww"
      },
      "source": [
        "#Lets apply logistic regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[[\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]]\n",
        "y = df[\"income\"]\n",
        "\n",
        "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UaZhMW1jWbb",
        "outputId": "0df81fa5-d7da-409f-9eb9-edce3498fba6"
      },
      "source": [
        "#Training model with Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train_2 = sc.fit_transform(x_train_2) \n",
        "x_test_2 = sc.fit_transform(x_test_2) \n",
        "\n",
        "model_2 = LogisticRegression(random_state = 0) \n",
        "model_2.fit(x_train_2, y_train_2)\n",
        "\n",
        "y_pred_2 = model_2.predict(x_test_2)\n",
        "\n",
        "accuracy = accuracy_score(y_test_2, y_pred_2)\n",
        "print(accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8116929064213692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je_NNNgijjMO"
      },
      "source": [
        "0.8116929064213692\n",
        "\n",
        "With Logistic Regression, this time, we got an accuracy of 81.1%. Let's study this more closely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpHfUNP2jqe9"
      },
      "source": [
        "Difference b/w Naive Bayes and Logistic Regression\n",
        "\n",
        "\\\n",
        " In the first dataset, as we pointed out earlier, both the glucose and the bloodpressure had little correlation, and both of them were contributing individually to whether a person has diabetes or not.\n",
        "\n",
        "\\\n",
        " Conclusion In these kinds of dataset, where all the features contribute individually to the outcome, Naive Bayes outperforms logistic regression and is highly efficient.\n",
        "\n",
        "\\\n",
        "In the second dataset, Logistic Regression outperformed Naive Bayes. The reason is that in this dataset, not all features contribute individually to the outcome. For example, there have been people of all age groups earning both less than and more than 50K. There have also been people with all education numbers that have an income of both less and more than 50K. Here, the combination of all the features is a better predictor of whether a person is earning more than or less than 50K, instead of all features having their individual contribution."
      ]
    }
  ]
}